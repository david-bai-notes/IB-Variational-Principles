\section{Extensions of the Euler-Lagrange Equations}
\subsection{Euler-Lagrange with Constraints}
Our objective is to extremize the functional
$$F[y]=\int_\alpha^\beta f(x,y,y^\prime)\,\mathrm dx$$
subject to the constraint $G[y]=0$ for a functional $G$ in the form
$$G[y]=\int_\alpha^\beta g(x,y,y^\prime)\,\mathrm dx$$
We can tackle this by using an analog of Lagrange multiplier.
Consider the new functional
$$\Phi[y;\lambda]=F[y]-\lambda G[y]=\int_\alpha^\beta (f-\lambda g)(x,y,y^\prime)\,\mathrm dx$$
from the study of Lagrange multiplier earlier in the $\mathbb R^n$ case, we are inspired to extremise $\Phi$ instead.
The Euler-Lagrange equation form $\Phi$ is then
$$\frac{\mathrm d}{\mathrm dx}\frac{\partial}{\partial y^\prime}(f-\lambda g)=\frac{\partial}{\partial y}(f-\lambda g)$$
\begin{example}[Dido's Problem (aka the Isoperimetric Problem)]
    We want to ask what simple closed plane curve with fixed length $L$ maximises its area.
    We can assume WLOG that the curve is convex and put it on the coordinate plane.
    Then by convexity it is bounded by the lines $x=\alpha,x=\beta$ for some $\alpha,\beta$.
    Also, for each $x\in(\alpha,\beta)$ there are exactly two values $y=y_1,y_2$, $y_1<y_2$ such that $(x,y)$ is on the curve.
    The area element is then $\mathrm dA=(y_2-y_1)\,\mathrm dx$.
    So the functional we want to maximise is
    $$A[y]=\int_\alpha^\beta y_2-y_1\,\mathrm dx=\oint_Cy\,\mathrm dx$$
    subject to the contraint that
    $$L[y]=\oint_C\,\mathrm dl=\oint_C\sqrt{1+(y^\prime)^2}\,\mathrm dx$$
    is constantly $L$.
    To use Lagrange multiplier, we set $h=y-\lambda\sqrt{1+(y^\prime)^2}$, then as $h$ does not explicitly depend on $x$, we can use the first integral
    $$K=\text{const.}=h-y^\prime\frac{\partial h}{\partial y^\prime}=y-\frac{\lambda}{\sqrt{1+(y^\prime)^2}}\implies (y^\prime)^2=\frac{\lambda^2}{(y-K)^2}-1$$
    Hence,
    $$\int\frac{y-K}{\sqrt{\lambda^2-(y-K)^2}}\,\mathrm dy=x-x_0\implies (y-y_0)^2+(x-x_0)^2=\lambda^2$$
    where $x_0,y_0$ are constants.
    This is the equation of a circle, and by the constraint, $\lambda=L/2\pi$.
\end{example}
\begin{example}[The Sturm-Liouville Problem]\label{sturm-liouville}
    Let $\rho=\rho(x)>0$ for $x\in [\alpha,\beta]$.
    Consider the following functional:
    $$F[y]=\int_\alpha^\beta\rho(x)(y^\prime)^2+\sigma(x)y^2\,\mathrm dx$$
    which we want to maximise subject to the condition that
    $$G[y]=\int_\alpha^\beta y^2\,\mathrm dx=1$$
    One will see it again and again in the settings of quantum mechanics.
    So our goal is to extremise
    $$\Phi[y;\lambda]=F[y]-\lambda(G[y]-1)$$
    So
    $$h=\rho(y^\prime)^2+\sigma y^2-\lambda\left( y^2-\frac{1}{\beta-\alpha} \right)$$
    The Euler-Lagrange equation is then
    $$-\frac{\mathrm d}{\mathrm dx}(\rho y^\prime)+\sigma y=\lambda y$$
    We write $\mathcal L(y)$ to denote the differential operator on the right hand side.
    $\mathcal L$ is called the Sturm-Liouville operator.
    Viewing it like this, the ODE is now the eigenvalue problem of the operator $\mathcal L$.
\end{example}
\begin{remark}
    If $\rho=1$, then $\sigma$ can be taken as the potential which makes the equation the (one-dimensional) time-independent Schr\"odinger equation.
\end{remark}
If $\sigma>0$ everywhere, then $F[y]>0$ everywhere.
\begin{claim}
    We claim that he (positive) mimimum of $F[y]$ is the lowest eigenvalue of $\mathcal L$.
\end{claim}
\begin{proof}
    We multiply the Sturm-Liouville equation by $y$ on both sides and integrate from $\alpha$ to $\beta$, which gives $F[y]=\lambda G[y]$.
\end{proof}
\subsection{Several Dependent Variables}
Suppose $\underline{y}(x)=(y_1(x),\ldots,y_n(x))$.
We want to consider the functional
$$F[\underline{y}]=\int_\alpha^\beta f(x,y_1,\ldots,y_n,y_1^\prime,\ldots,y_n^\prime)\,\mathrm dx$$
Suppose a perturbation $y_i=y_i+\epsilon \eta_i$ is incurred for $i=1,\ldots,n$, $\epsilon>0$ small and $\eta_i(\alpha)=\eta_i(\beta)=0$.
Then
$$F[\underline{y}+\epsilon\underline{\eta}]-F[\underline{y}]=\epsilon\int_\alpha^\beta\sum_{i=1}^n\eta_i\left( \frac{\mathrm d}{\mathrm dx}\frac{\partial f}{\partial y_i^\prime}-\frac{\partial f}{\partial y_i} \right)\,\mathrm dx+O(\epsilon^2)$$
By Lemma \ref{fund_lemma}, we have
$$\frac{\mathrm d}{\mathrm dx}\frac{\partial f}{\partial y_i^\prime}=\frac{\partial f}{\partial y_i}$$
for $i=1,\ldots,n$.
This is simply just $n$ Euler-Lagrange equations, which are $n$ second-order ODEs.
Of course, we also want to reduce this system of second order ODEs to first integrals whenever possible.\\
If $\partial f/\partial y_j=0$ for some $j$, then
$$\frac{\partial f}{\partial y_j^\prime}=\text{const.}$$
If the number of such $j$'s is big enough, we can simplify this a big deal.\\
If $\partial f/\partial x=0$, then we have
$$f-\sum_{i=1}^ny_i^\prime\frac{\partial f}{\partial y_i^\prime}=\text{const.}$$
which might help in some cases.
\begin{example}[Geodesics on Surfaces]
    Consider a surface $\Sigma\subset\mathbb R^3$ given by $g(x,y,z)=0$.
    Pick two points $A,B$ from $\Sigma$.
    Our aim is to minimize the length of shortest path on $\Sigma$ which connects $A,B$.
    If such a length exists, then any path with this length is called a geodesics.
    Let $t\in[0,1]$ be a parameter on the curve and parameterise a curve between $A,B$ by $\underline{x}(0)=A,\underline{x}(1)=B$.
    For the curve to lie on the surface, we have the constraint $g(\underline{x}(t))=0$ for any $t$.\\
    So by the idea of Lagrange multiplier, we want to minimise the functional
    $$\Phi[\underline{x},\lambda]=\int_0^1\left( \sqrt{\dot{x}^2+\dot{y}^2+\dot{z}^2} -\lambda(t) g(x,y,z)\right)\,\mathrm dt$$
    Note that the Lagrange multiplier is now a function $\lambda=\lambda(t)$, as our constraint has to be valid for all $t$.
    Write the integrand in $\Phi$ by $h(x,y,z,\lambda,\dot{x},\dot{y},\dot{z})$.
    The Euler-Lagrange equation wrt $\lambda$ is then
    $$\frac{\mathrm d}{\mathrm dt}\frac{\partial h}{\partial \dot{\lambda}}-\frac{\partial h}{\partial\lambda}=0\implies g(x,y,z)=-\frac{\partial h}{\partial\lambda}=0$$
    For the rest of the Euler-Lagrange equations, write $(x,y,z)=x_i\underline{e_i}$, then it becomes
    $$\frac{\mathrm d}{\mathrm dt}\left( \frac{\dot{x}_i}{\sqrt{\dot{x}^2+\dot{y}^2+\dot{z}^2}} \right)+\lambda\frac{\partial g}{\partial x_i}=0$$
    which, for specified $g$, should solve to get us a geodesics.
\end{example}
\subsection{Several Independent Variables}
In general, the parameter of the functional is a function $\mathbb R^n\to\mathbb R^m$.
If $n>1$, then the Euler-Lagrange equations, as one may expect, become PDEs.
Suppose $n=3$, then the functional can have the form
$$F[\phi]=\int_Df(x,y,z,\phi,\phi_x,\phi_y,\phi_z)\,\mathrm dx\,\mathrm dy\,\mathrm dz$$
where $D\subset\mathbb R^3$.
Assume $\phi$ is at an extremum of $F$, then the perturbation we are considering would become
$$\phi(x,y,z)\mapsto \phi(x,y,z)+\epsilon \eta(x,y,z), \epsilon\in\mathbb R,\eta|_{\partial D}=0$$
So if we write
$$\underline{v}=\left( \frac{\partial f}{\partial \phi_x},\frac{\partial f}{\partial \phi_y},\frac{\partial f}{\partial\phi_z} \right)$$
then,
\begin{align*}
    F[\phi+\epsilon\eta]-F[\phi]&=\epsilon\int_D\left( \eta\frac{\partial f}{\partial\phi}+\eta_x\frac{\partial f}{\partial \phi_x}+\eta_y\frac{\partial f}{\partial \phi_y}+\eta_z\frac{\partial f}{\partial \phi_z} \right)\,\mathrm dx\,\mathrm dy\,\mathrm dz+O(\epsilon^2)\\
    &=\epsilon\int_D\left( \eta\frac{\partial f}{\partial\phi}+\nabla\cdot( \eta\underline{v})-\eta\nabla\cdot\underline{v} \right)\,\mathrm dx\,\mathrm dy\,\mathrm dz+O(\epsilon^2)
\end{align*}
By the Divergence Theorem, we have
$$\int_D\nabla\cdot(\eta\underline{v})\,\mathrm dx\,\mathrm dy\,\mathrm dz=\int_{\partial D}\eta\underline{v}\cdot\mathrm d\underline{S}=0$$
By the boundary assumption on $\eta$, therefore
$$\frac{F[\phi+\epsilon\eta]-F[\phi]}{\epsilon}=\int_D\eta\left( \frac{\partial f}{\partial\phi}-\nabla\cdot\underline{v} \right)\,\mathrm dx\,\mathrm dy\,\mathrm dz$$
We obviously have the analogy of Lemma \ref{fund_lemma} here, hence for $\delta F=0$, we obtain
$$\frac{\partial f}{\partial\phi}-\nabla\cdot\underline{v}=0$$
which, by expanding $\underline{v}$, is
$$\frac{\partial f}{\partial\phi}-\frac{\partial}{\partial x_i}\frac{\partial f}{\partial \phi_{x_i}}=0$$
where the summation is implied by convention.
This can be generalized from $3$ to $n$ in an obvious way.
\begin{example}
    We want to extremise the functional of potential energy
    $$F[\phi]=\iint_{D\subset\mathbb R^2}\frac{1}{2}(\phi_x^2+\phi_y^2)\,\mathrm dx\,\mathrm dy$$
    Where $\phi:\mathbb R^2\to\mathbb R$.
    So the Euler-Lagrange equation becomes
    $$\phi_{xx}+\phi_{yy}=0$$
    which is the Laplace equation.
\end{example}
\begin{example}[Minimal Surfaces]
    We want to minimise the area of a surface $\Sigma\subset\mathbb R^3$ subject to boundary conditions (e.g. specified $\partial\Sigma$).
    This can allow us to find the shape of a soap film.
    Suppose we can write $\Sigma$ as the graph of $z=f(x,y)$ for some function $\phi$.
    \footnote{This can always be done locally given that $\Sigma$ is nice enough due to Implicit Function Theorem.}
    The line element is $\mathrm ds^2=\mathrm dx^2+\mathrm dy^2+\mathrm dz^2$ and we have $\mathrm dz=\phi_x\,\mathrm dx+\phi_y\,\mathrm dy$, so
    $$\mathrm ds^2=(1+\phi_x^2)\,\mathrm dx^2+(1+\phi_y^2)\mathrm dy^2+2\phi_x\phi_y\,\mathrm dx\,\mathrm dy$$
    This is called the first fundamental form (or the Riemannian metric) in geometrical settings.
    We can write, in summation notation, $\mathrm ds^2=g_{ij}(x,y)\,\mathrm dx^i\,\mathrm dx^j$ with
    $$g=\begin{pmatrix}
            1+\phi_x^2&\phi_x\phi_y\\
            \phi_x\phi_y&1+\phi_y^2
    \end{pmatrix}$$
    As one can verify, $\det g\ge 0$ and $\sqrt{\det g}$, so
    $$A[\phi]=\iint_D\sqrt{1+\phi_x^2+\phi_y^2}\,\mathrm dx\,\mathrm dy$$
    Write $h$ to denote the integrand, then we have
    $$\frac{\partial h}{\partial\phi_x}=\frac{\phi_x}{\sqrt{1+\phi_x^2+\phi_y^2}},\frac{\partial h}{\partial\phi_y}=\frac{\phi_y}{\sqrt{1+\phi_x^2+\phi_y^2}}$$
    So, upon some simplification, the Euler-Lagrange equation transforms to
    $$(1+\phi_y^2)\phi_{xx}+(1+\phi_x^2)\phi_{yy}-2\phi_x\phi_y\phi_{xy}=0$$
    which is known as the minimal surface equation.\\
    A particular case of it is when the surface is a surface of revolution, i.e. $z=z(r),r=\sqrt{x^2+y^2}$.
    We want to solve it subject to the surface's boundary being two equal circles whose centres are on the $z$-axis and are parallel to the $x-y$ plane.
    This initial condition can give us the shape of soap film between two circular loops.\\
    In this case, the PDE transforms to
    $$rz^{\prime\prime}+z^\prime+(z^\prime)^3$$
    by a lot of calculations.
    Write $w=z^\prime$, then
    $$\frac{1}{2}r\frac{\mathrm dw^2}{\mathrm dr}+w^2+w^4$$
    which we can integrate to find
    $$r=r_0\cosh\left( \frac{z-z_0}{r_0} \right)$$
    which is called the catenoid.
    This result was first proved by Euler in 1744.
    Note that being a catenoid is a necessary condition for a minimal surface in this form to exist.\\
    Suppose the centres of the circles are located at $L\underline{e_z}$ and $-L\underline{e_z}$ and both circles have radius $R$.
    Then $r(L)=r(-L)$, hence if $L\neq 0$ then $-L-z_0=-L+z_0$, thus $z_0=0$.
    Therefore $R=r_0\cosh(L/r_0)$.
    But $r_0$ is then the radius of the circle as the intersection between the surface and the $x-y$ plane.
    WLOG set $L=1$ (as we can scale the whole thing), then by a simple plot we obtain that $R\ge c$ globally for some $c>0$.
    By estimation the minimum occurs at $r_0\approx 0.833$ and $c\approx 1.5$.
    Also, for $R>c$, we see that there are two possible values of $r_0$, which corresponds to two minimal surfaces.
    After our latter discussion on second variations, we find that the thinner one of which is unstable and the thicker one is stable.
\end{example}
The case where $R$ is less than the threshold exhibits an example where there is no solution.
\subsection{Higher Derivatives}
We want to generalise the theory to a wider range of functionals, specifically in the form
$$F[y]=\int_\alpha^\beta f(x,y,y^\prime,\ldots,y^{(n)})\,\mathrm dx$$
Assume such an $y$ that extremises $F$ exists, then consider the perturbation $y\mapsto y+\epsilon\eta$ where $\eta,\eta^\prime,\ldots,\eta^{(n-1)}$ all vanishes at $\alpha,\beta$, then by the higher-dimensional Taylor expansion,
$$F[y+\epsilon\eta]-F[y]=\epsilon\int_\alpha^\beta\left( \frac{\partial f}{\partial y}\eta+\cdots +\frac{\partial f}{\partial y^{(n)}}\eta^{(n)} \right)\,\mathrm dx+O(\epsilon^2)$$
By integration by part,
$$\int_\alpha^\beta\frac{\partial f}{\partial y^{(i)}}\eta^{(i)}\,\mathrm dx=\int_\alpha^\beta\eta(-1)^i\frac{\mathrm d^i}{\mathrm dx^i}\frac{\partial f}{\partial y^{(i)}}\,\mathrm dx$$
Due to our boundary conditions on $\eta$.
Hence
$$\frac{F[y+\epsilon\eta]-F[y]}{\epsilon}=\int_\alpha^\beta \eta\left(\frac{\partial f}{\partial y}+\sum_{i=1}^n(-1)^i\frac{\mathrm d^i}{\mathrm dx^i}\frac{\partial f}{\partial y^{(i)}}\right)\,\mathrm dx+O(\epsilon)$$
Hence by Lemma \ref{fund_lemma}, we obtain the Euler-Lagrange equation in the following form:
$$\frac{\partial f}{\partial y}+\sum_{i=1}^n(-1)^i\frac{\mathrm d^i}{\mathrm dx^i}\frac{\partial f}{\partial y^{(i)}}=0$$
\begin{example}[First Integral]
    If $n=2$ and $f$ does not explicitly depend on $y$, the equation becomes
    $$\frac{\mathrm d}{\mathrm dx}\left( \frac{\partial f}{\partial y^\prime}-\frac{\mathrm d}{\mathrm dx}\frac{\partial f}{\partial y^{\prime\prime}} \right)=0\implies \frac{\partial f}{\partial y^\prime}-\frac{\mathrm d}{\mathrm dx}\frac{\partial f}{\partial y^{\prime\prime}}=\text{const.}$$
\end{example}
\begin{example}
    If we want to extremise
    $$F[y]=\int_0^1(y^{\prime\prime})^2\,\mathrm dx$$
    with $y(0)=y(1)=y^\prime(0)=0$ and $y^\prime(1)=1$.
    Then the first integral above reduces to
    $$\frac{\mathrm d}{\mathrm dx}(2y^{\prime\prime})=\text{const.}\implies y^{(3)}=\text{const.}$$
    which solves to $y=x^3-x^2$ for the specified boundary conditions.
\end{example}
Note that the $y_0$ in the previous example is actually an absolute minimum of the functional $F$.
To argue this, observe that for an arbitrary $C^2$ function $\eta$ with $\eta,\eta^\prime$ both vanish at $0,1$ but $\eta$ is not identically zero.
Then
\begin{align*}
    F[y_0+\eta]-F[y_0]&=\int_0^1(\eta^{\prime\prime})^2\,\mathrm dx+2\int_0^1 y_0^{\prime\prime}\eta^{\prime\prime}\,\mathrm dx\\
    &>4\int_0^1(3x-1)\eta^{\prime\prime}\\
    &=0
\end{align*}
through integration by part.
Hence $y$ is the global minimum in our chosen space $C^2[0,1]$.
This trick does not always work, but sometimes it does, like the one we just did.