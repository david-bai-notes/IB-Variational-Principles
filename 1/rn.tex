\section{Calculus in Real Vector Spaces}
\subsection{Stationary Points}
Let $C^2(\mathbb R^2)$ denote the collection of twice differentiable functions $\mathbb R^n\to\mathbb R$ with continuous second derivatives.
\begin{definition}
    Let $f\in C^2(\mathbb R^n)$.
    A point $\underline{a}\in\mathbb R^n$ is stationary (or is a stationary point of $f$) if $\nabla f(\underline{a})=\underline{0}$.
\end{definition}
If we expand $f$ near one of its stationary points $\underline{x}=\underline{a}$, then
\begin{align*}
    f(\underline{x})&=f(\underline{a})+(\underline{x}-\underline{a})\cdot\nabla f+\frac{1}{2}(x_i-a_i)(x_j-a_j)\frac{\partial^2f}{\partial x_i\partial x_j}(\underline{a})+o(|\underline{x}-\underline{a}|^2)\\
    &=f(\underline{a})+\frac{1}{2}(\underline{x}-\underline{a})^\top H(\underline{a})(\underline{x}-\underline{a})
\end{align*}
Where $H_{ij}(\underline{a})=\partial^2f/\partial x_i\partial x_j(\underline{a})$ is the Hessian, which is a real symmetric matrix for every $\underline{a}$.\\
Shift the origin to set $\underline{a}=\underline{0}$ and write $H=H(\underline{0})$.
As $H$ is real symmetric, we know that in another orthonormal basis $H$ has the form $H=\operatorname{diag}(\lambda_1,\ldots,\lambda_n)$, hence in this new coordinate system,
$$f(\underline{x})-f(\underline{0})=\frac{1}{2}\sum_{i=1}^n\lambda_ix_i^2+o(|x|^2)$$
Therefore,
\begin{theorem}
    Let $\lambda_i$ as before, then if $\forall i,\lambda_i>0$, then $\underline{0}$ is a local minimum.\\
    If instead $\forall i,\lambda_i<0$, then $\underline{0}$ is a local maximum.\\
    If some of the eigenvalues are positive and some are negative, then it is a saddle point.\\
    If some $i$ has $\lambda_i=0$, 
\end{theorem}
If some $i$ has $\lambda_i=0$, we have to look at higher order terms in the Taylor series.
\begin{proof}
    Trivial.
\end{proof}
In the special case where $n=2$, we have $\det H=\lambda_1\lambda_2$ and $\operatorname{tr} H=\lambda_1+\lambda_2$, both of which are easier to calculate than the actual eigenvalues, so we can formulate a corollary to make things easier.
\begin{corollary}
    If $\det H>0,\operatorname{tr}H>0$, then it is a local minimum.\\
    If $\det H>0,\operatorname{tr}H<0$, it is a local maximum.\\
    If $\det H<0$, it is a saddle point.
\end{corollary}
\begin{proof}
    Obvious.
\end{proof}
Despite these results, one should note that the actual (global) maxima and minima might occur at boundaries where we may not have $\nabla f=0$.\\
Also, there are plenty of examples of functions who only have saddle points.
Consider a harmonic function $f$ on $D\subset\mathbb R^2$, i.e. $f_{xx}+f_{yy}=0$, then at a stationary point we necessarily have $\operatorname{tr}H=0$, hence either $\det H=0$ or it is a saddle point.\\
But of course, the theorem and corollary can help a lot in classifying stationary points.
\begin{example}
    Let $f(x,y)=x^3+y^3-3xy$, then $\nabla f=(3x^2-3y,3y^2-3x)^\top$, so the only stationary points are $(x,y)=(0,0),(1,1)$.
    $$H=\begin{pmatrix}
        6x&-3\\
        -3&6y
    \end{pmatrix}$$
    Hence, by the corollary, $(0,0)$ is a saddle point while $(1,1)$ is a minimum.
    One can easily produce a phase diagram using these information.
\end{example}
\subsection{Constraints and Lagrange Multipliers}
The optimisation problems in $\mathbb R^n$ are most likely coming with some sort of constraints.
We first illustrate this by way of an example
\begin{example}
    We want to find a circle centered at $(0,0)$ with smallest radius which intersects the parabola $y=x^2-1$.
    Naturally, there are many ways to do this.
    One can solve the problem directly by simply substitution
    $$x^2+y^2=x^2+(x^2-1)^2=x^4-x^2+1$$
    which minimum is $3/4$ by either calculus or completing square.
    So the smallest radius is $\sqrt{3}/2$.\\
    The second method to solve this probelm, which are are going to use, is called the Lagrange multipliers.
    Define a new function
    $$h(x,y,\lambda)=f(x,y)-\lambda g(x,y)$$
    where $f(x,y)=x^2+y^2$ is the function we want to minimise and $g(x,y)=y-x^2+1$ which vanishes if $(x,y)$ is on the parabola.
    $\lambda$ here is called the Lagrange multiplier.
    We now try to extremise $h$ over $x,y,\lambda$.
    Naturally, we take the particial derivatives
    $$\begin{cases}
        \partial h/\partial x=2x+2\lambda x\\
        \partial h/\partial y=2y-\lambda\\
        \partial h/\partial\lambda=y-x^2+1
    \end{cases}$$
    Hence the stationary point of $h$ are at $(x,y)=(0,-1),(\pm 1/\sqrt{2},-1/2)$.
    Plugging in yields the same solution.
\end{example}
Why does the method of Lagrange multiplier work?
Geometrically, if we want to minimise a function $f$ subject to $g=0$.
Now $\nabla g$ is always perpendicular to $g=0$.
Then, suppose the minimum of $f$ is $c$, then the graph of $f(x)=c$ would touch $g(x)=0$, therefore their gradients there are parallel, so $\nabla f=\lambda\nabla g$ for some $\lambda$, hence $\nabla h=0$ (note that $\partial h/\partial\lambda=0$ iff $g=0$), which is exactly the system we want to solve in the last part.\\
If there are many constraints $g_\alpha(\underline{x})=0$ of the minimisation problem of $f:\mathbb R^k\to\mathbb R$ where $\alpha$ ranges from $1$ to $k$, we can define an analogous
$$h(x_1,\ldots,x_k,\lambda_1,\ldots,\lambda_k)=f(\underline{x})-\lambda_\alpha g_\alpha(\underline{x})$$
then finding the stationary points of $h$ can help to determine the minimisation problem.